---
general_params:
  env: dev
  abs_path_to_airflow2_dags: # e.g., <path-to>/airflow-v2-dags. Note: make sure you don't include a trailing backslash.
db_connection_params:
  snowflake_username: # e.g., jbloggs@email.com
  abs_path_to_snowflake_private_key: # e.g., /<abs-path-to->/rsa-key.p8
  snowflake_account: <snowflake_account>.ap-southeast-2
  snowflake_warehouse: # e.g., wh_svc_dbt_xs_${ENV}
  snowflake_role: # e.g., svc_dbt_${ENV}
  snowflake_src_db: # e.g., ${DATA_SRC}_${ENV}
  snowflake_src_db_schema: # e.g., landed
  snowflake_db_schema_incremental: # e.g., 'incremental'
dbt_params:
  dbt_version: 1.2.0
  dbt_project_name: example_generated_dbt_project # e.g., dbt_${DATA_SRC} # name of the dbt project. Must be snake_case, i.e., letters, digits and underscores only, and cannot start with a digit.
  dbt_profile_name: # The profile your dbt project should use to connect to your data warehouse, must be snake_case.
  # Recommendation, re dbt_profile_name: Often an organisation has only one data warehouse, so it makes sense to use your organisation's name as a profile name.
  dbt_model: staging # target data model to land the generated SQL files
ip_metadata_file_params:
  table_level_metadata: ip/table_level_metadata.xlsx # table-level metadata, used to capture the 'primary_key' & 'last_updated' for each src table across the data sources.
  metadata_sheet_name: Sheet1 # XLS sheet name
data_src_params:
  data_src: # e.g. data_src_a
  data_src_tables:
    data_src_a_src_tables: # data source source tables (used for the 2 .py generation scripts)
    - table_a
    - table_b
